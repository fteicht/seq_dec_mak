{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning graded exercice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment simulation\n",
    "\n",
    "We will start with a simple deterministic control environment that we will make stochastic in a second phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple line control environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with a very simple control problem consisting in maintaining a moving object along a straight line.\n",
    "We can control its acceleration as shown in the following picture.\n",
    "The objective is to keep minimum the distance between the object and the center line as long as possible.\n",
    "To make the problem a bit interesting, we constrain the norm of the acceleration to be larger than a given value $a_{min}$: $\\forall t > 0, \\Vert a(t) \\Vert_2 \\geqslant a_{min}$.\n",
    "\n",
    "![Line control environment](line_control_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical modeling of the Reinforcement Learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to learn to control the object using Reinforcement Learning.\n",
    "The state of the system must contain the minimal information required to update the physics of the system from the action, i.e. the acceleration of the object.\n",
    "In this case, we need at least the position and the speed of the object, thus the state will be defined at any time $t$ by $s(t)=(x(t), y(t), v_x(t), v_y(t))$\n",
    "Moreover, for classical RL algorithms to apply, we need to discretize the time every $\\Delta t$ time units.\n",
    "We can now approximate the physics of the object movement with the following equations, knowing the initial state $s(0)=(x(0), y(0), v_x(0), v_y(0))$:\n",
    "- $\\forall t > 0, v_x(t+\\Delta t) = v_x(t) + a_x(t) \\cdot \\Delta t$ ;\n",
    "- $\\forall t > 0, x(t+\\Delta t) = x(t) + v_x(t) \\cdot \\Delta t$ ;\n",
    "- $\\forall t > 0, v_y(t+\\Delta t) = v_y(t) + a_y(t) \\cdot \\Delta t$ ;\n",
    "- $\\forall t > 0, y(t+\\Delta t) = y(t) + v_y(t) \\cdot \\Delta t$ ;\n",
    "\n",
    "Since we want to keep minimum the distance between the center line and the object, we will model the reward signal at any time $t$ by $r(t) = e^{-\\vert y(t) \\vert}$.\n",
    "By doing so, an RL agent who will try to maximize the cumulated sum of (discounted) rewards will try to keep $y(t)$ as close as possible to $0$ at any time step.\n",
    "There are two possible ways to enforce the constraint $\\Vert a(t) \\Vert_2 \\geqslant a_{min}$ in RL:\n",
    "- either by ensuring that the algorithm will only select such actions ;\n",
    "- or by associating a very large penalty (i.e. negative reward) to transitions labelled with such actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation in a Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OpenAI Gym](https://gym.openai.com/) is a popular Python software library to model RL environments in a standard way which can be exploited by RL algorithm libraries like [RLlib](https://www.ray.io/rllib) or [Stable Baselines](https://github.com/DLR-RM/stable-baselines3).\n",
    "\n",
    "OpenAI Gym - or Gym in short - provides well-known environment implementations like CartPole, but we can also implement our own environment by following their standards, which will allow us to solve our environment using well-implemented and efficient RL algorithms from the aforementioned libraries. We will use the open-source Airbus library [scikit-decide](https://github.com/airbus/scikit-decide) which will allow us to use either RLLib or Stable Baselines as underlying solvers.\n",
    "\n",
    "All we have to do is to implement a domain class with the following methods:\n",
    "```python\n",
    "class State:\n",
    "    # Define your state class here\n",
    "    pass\n",
    "\n",
    "class Action:\n",
    "    # Define your action class here\n",
    "    pass\n",
    "\n",
    "\n",
    "class D(RLDomain, UnrestrictedActions, FullyObservable, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "class MyDomain(D):\n",
    "    def __init__(self):\n",
    "        # Declare your variables here, including the environment's state.\n",
    "        # Declare also the action and observation (i.e. state) spaces: the action space is used by the algorithm\n",
    "        # to select actions while the observation space is used by Deep RL algorithms to properly initialize\n",
    "        # the observation (i.e. state) layer of the tensors.\n",
    "        pass\n",
    "\n",
    "    def _state_reset(self) -> D.T_state:\n",
    "        # Initialize and return the initial state of the environment\n",
    "        pass\n",
    "\n",
    "    def _state_step(self, action: D.T_event) -> TransitionOutcome[D.T_state, Value[D.T_value], D.T_predicate, D.T_info]:\n",
    "        # Perform one simulation step of the environment, i.e. compute the state resulting from applying the given action in the current state.\n",
    "        # Don't forget to update the environment's state so that the next call to the step method will reason about the updated state.\n",
    "        # Must return a tuple (state, reward, done, info) where done is true if the episode should stop now and info is a dictionary that can be left empty.\n",
    "        pass\n",
    "\n",
    "    def  _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "\n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        pass\n",
    "\n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        # If you want to render something at each simulation step (e.g. an image, some text, etc.)\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's your turn!\n",
    "Please fill in the missing lines in the definition below of the Gym environment which implements \"simple line control\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "from skdecide.hub.space.gym import *\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "from math import sqrt, exp, fabs\n",
    "\n",
    "\n",
    "HORIZON = 500\n",
    "ACCELERATION_MIN = 0.5\n",
    "PENALTY = -1000.\n",
    "\n",
    "\n",
    "class D(RLDomain, UnrestrictedActions, FullyObservable, Renderable):\n",
    "    T_state = ArrayLike  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = ArrayLike  # Type of events\n",
    "    T_action = T_event\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class SimpleLineControlDomain(D):\n",
    "    \"\"\"This class mimics an OpenAI Gym environment\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 50}\n",
    "\n",
    "    def __init__(self, env_config=None):\n",
    "        \"\"\"Initialize GymDomain.\n",
    "        # Parameters\n",
    "        gym_env: The Gym environment (gym.env) to wrap.\n",
    "        \"\"\"\n",
    "        inf = np.finfo(np.float32).max\n",
    "        self.action_space = BoxSpace(\n",
    "            np.array([-1.0, -1.0]), np.array([1.0, 1.0]), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = BoxSpace(\n",
    "            np.array([-inf, -inf, -inf, -inf]),\n",
    "            np.array([inf, inf, inf, inf]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self._delta_t = 0.001\n",
    "        self._init_pos_x = 0.0\n",
    "        self._init_pos_y = 0.5\n",
    "        self._init_speed_x = 10.0\n",
    "        self._init_speed_y = 1.0\n",
    "        self._pos_x = None\n",
    "        self._pos_y = None\n",
    "        self._speed_x = None\n",
    "        self._speed_y = None\n",
    "        self._path = []\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.array(\n",
    "            [self._pos_x, self._pos_y, self._speed_x, self._speed_y], dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self._pos_x = state[0]\n",
    "        self._pos_y = state[1]\n",
    "        self._speed_x = state[2]\n",
    "        self._speed_y = state[3]\n",
    "\n",
    "    def _state_reset(self) -> D.T_state:\n",
    "        self._pos_x = self._init_pos_x\n",
    "        self._pos_y = self._init_pos_y\n",
    "        self._speed_x = self._init_speed_x\n",
    "        self._speed_y = self._init_speed_y\n",
    "        self._path = []\n",
    "        return np.array(\n",
    "            [self._pos_x, self._pos_y, self._speed_x, self._speed_y], dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def _state_step(\n",
    "        self, action: D.T_event\n",
    "    ) -> TransitionOutcome[D.T_state, Value[D.T_value], D.T_predicate, D.T_info]:\n",
    "        ### WRITE YOUR CODE HERE ###\n",
    "\n",
    "        ############################\n",
    "        self._path.append((self._pos_x, self._pos_y))\n",
    "        return TransitionOutcome(obs, Value(reward=reward if not done else PENALTY), done, {})\n",
    "\n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        return self.observation_space\n",
    "\n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        return self.action_space\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "\n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.surf = pygame.Surface((screen_width, screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "        self.track = gfxdraw.hline(\n",
    "            self.surf, 0, screen_width, int(screen_height / 2), (0, 0, 255)\n",
    "        )\n",
    "\n",
    "        if len(self._path) > 1:\n",
    "            for p in range(len(self._path) - 1):\n",
    "                gfxdraw.line(\n",
    "                    self.surf,\n",
    "                    int(self._path[p][0] * 100),\n",
    "                    int(screen_height / 2 + self._path[p][1] * 100),\n",
    "                    int(self._path[p + 1][0] * 100),\n",
    "                    int(screen_height / 2 + self._path[p + 1][1] * 100),\n",
    "                    (255, 0, 0),\n",
    "                )\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        return np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test a trajectory of a random RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "from skdecide.builders.solver import *\n",
    "\n",
    "\n",
    "class D(RLDomain, UnrestrictedActions, FullyObservable, Renderable):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RandomSolver(DeterministicPolicySolver):\n",
    "    T_domain = D\n",
    "    \n",
    "    def __init__(self, action_space) -> None:\n",
    "        super().__init__()\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def _get_next_action(self, observation: D.T_observation) -> D.T_event:\n",
    "        return np.random.Generator.choice(self.action_space.sample())\n",
    "\n",
    "    def _is_policy_defined_for(self, observation: D.T_observation) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "domain_factory = lambda: SimpleLineControlDomain()\n",
    "domain = domain_factory()\n",
    "\n",
    "def rollout(domain, solver, max_steps):\n",
    "    obs = domain.reset()\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        plt.imshow(domain.render())\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        obs, reward, done, info = domain.step(\n",
    "            solver.sample_action()\n",
    "        )\n",
    "\n",
    "rollout(domain=domain, solver=RandomSolver(domain.get_action_space()), max_steps=50)\n",
    "domain.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We optimize the RL agent using RLLib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RL algorithm (Trainer) we would like to use.\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from skdecide.hub.solver.ray_rllib import RayRLlib\n",
    "\n",
    "assert RayRLlib.check_domain(domain)\n",
    "solver_factory = lambda: RayRLlib(\n",
    "    PPO, train_iterations=100\n",
    ")\n",
    "\n",
    "with solver_factory() as solver:\n",
    "    # Solve domain\n",
    "    SimpleLineControlDomain.solve_with(solver, domain_factory)\n",
    "\n",
    "    # Test solution\n",
    "    rollout(\n",
    "        domain=domain,\n",
    "        solver=solver,\n",
    "        max_steps=200,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic line control"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the environment stochastic by assuming that the object actuators are noised in such a way that the lateral acceleration (along the y axis) follows a Gaussian distribution centered around the lateral acceleration command with a standard deviation depending on the magnitude of the longitudinal acceleration command.\n",
    "If we note $a^c_x(t)$ and $a^c_y(t)$ the acceleration command (RL actions), the actual accelerations that will act on the object are $a_x(t) = a^c_x(t)$ and $a_y(t) \\sim \\mathcal{N} \\left( a^c_y(t), \\sqrt{\\vert a^c_x(t) \\vert} \\right)$.\n",
    "Please implement this stochastic domain in a Gym environment and implement an RLLib agent that learns to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR TURN ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please experiment with another noised acceleration model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR TURN ###"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b81519d285639899e6ffbf246c769949a19cca29f480f5c915279ae591fdfb44"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
