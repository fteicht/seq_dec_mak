{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Reinforcement Learning problems by using scikit-decide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit-decide](https://github.com/airbus/scikit-decide) is an open-source library developed by Airbus to model and solve sequential decision-making problems such as reinforcement learning, planning and scheduling.\n",
    "The documentation of the library can be found [here](https://airbus.github.io/scikit-decide/).\n",
    "\n",
    "One benefit of scikit-decide relies on the opportunity to solve a problem of a given class, e.g. path planning problem, by using algorithms from different communities and to compare them, e.g. reinforcement learning and planning.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Solve classical RL benchmarks by using well-known RL frameworks which are directly accessible in scikit-decide.\n",
    "2. Solve planning problems by using the same RL frameworks, demonstrating the ability of the library to solve planning problems seen as RL problems.\n",
    "3. Solve an aircraft taxiing control problem by using RL algorithms.\n",
    "\n",
    "Below are the import libraries that we will use throughout this notebook, after having downloaded the potentially missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip --default-timeout=1000 install \"scikit-decide[all]\" \"shimmy[gym]\" pygame gym_jsbsim JSBSim==1.1.1 --use-pep517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Optional\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import folium\n",
    "from gym_jsbsim.catalogs.catalog import Catalog as prp\n",
    "from gym_jsbsim.envs.taxi_utils import *\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from ray.rllib.algorithms.dqn import DQN, DQNConfig\n",
    "\n",
    "from skdecide import Solver\n",
    "from skdecide.builders.domain.initialization import DeterministicInitialized\n",
    "from skdecide.hub.domain.gym import (\n",
    "    GymDiscreteActionDomain,\n",
    "    GymDomain,\n",
    "    GymDomainHashable\n",
    ")\n",
    "from skdecide.hub.domain.simple_grid_world import SimpleGridWorld\n",
    "from skdecide.hub.solver.stable_baselines import StableBaseline\n",
    "from skdecide.hub.solver.ray_rllib import RayRLlib\n",
    "from skdecide.hub.solver.astar import Astar\n",
    "from skdecide.hub.solver.mcts import UCT\n",
    "from skdecide.utils import rollout as skd_rollout\n",
    "\n",
    "# choose standard matplolib inline backend to render plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving classical RL benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the classical Cart Pole problem, as provided by Gymnasium and defined [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Gymnasium environment in scikit-decide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the gymnasium environment we would like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a domain factory using GymDomain proxy available in scikit-decide which will wrap the Gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_factory = lambda: GymDomain(gym.make(ENV_NAME, render_mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a screenshot of such an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = domain_factory()\n",
    "domain.reset()\n",
    "plt.imshow(domain.render())\n",
    "plt.axis(\"off\")\n",
    "domain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve with Reinforcement Learning (StableBaseline + PPO)\n",
    "\n",
    "We first try a solver coming from the Reinforcement Learning community that is make use of OpenAI [stable_baselines3](https://github.com/DLR-RM/stable-baselines3), which give access to a lot of RL algorithms.\n",
    "\n",
    "Here we choose [Proximal Policy Optimization (PPO)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) solver. It directly optimizes the weights of the policy network using stochastic gradient ascent. See more details in stable baselines [documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) and [original paper](https://arxiv.org/abs/1707.06347). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check compatibility\n",
    "We check the compatibility of the domain with the chosen solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = domain_factory()\n",
    "assert StableBaseline.check_domain(domain)\n",
    "domain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = StableBaseline(\n",
    "    PPO, \"MlpPolicy\", learn_config={\"total_timesteps\": 10000}, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training solver on domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GymDomain.solve_with(solver, domain_factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rolling out a solution\n",
    "\n",
    "We can use the trained solver to roll out an episode to see if this is actually solving the problem at hand.\n",
    "\n",
    "For educative purpose, we define here our own rollout (which will probably be needed if you want to actually use the solver in a real case). If you want to take a look at the (more complex) one already implemented in the library, see the `rollout()` function in [utils.py](https://github.com/airbus/scikit-decide/blob/master/skdecide/utils.py) module.\n",
    "\n",
    "By default, we display the solution in a matplotlib figure. If you need only to check whether the goal is reached or not, you can specify `render=False`. In this case, the rollout is greatly speed up and a message is still printed at the end of process specifying success or not, with the number of steps required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(\n",
    "    domain: GymDomain,\n",
    "    solver: Solver,\n",
    "    max_steps: int,\n",
    "    pause_between_steps: Optional[float] = 0.01,\n",
    "    render: bool = True,\n",
    "):\n",
    "    \"\"\"Roll out one episode in a domain according to the policy of a trained solver.\n",
    "\n",
    "    Args:\n",
    "        domain: the maze domain to solve\n",
    "        solver: a trained solver\n",
    "        max_steps: maximum number of steps allowed to reach the goal\n",
    "        pause_between_steps: time (s) paused between agent movements.\n",
    "          No pause if None.\n",
    "        render: if True, the rollout is rendered in a matplotlib figure as an animation;\n",
    "            if False, speed up a lot the rollout.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize episode\n",
    "    solver.reset()\n",
    "    observation = domain.reset()\n",
    "\n",
    "    # Initialize image\n",
    "    if render:\n",
    "        plt.ioff()\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.axis(\"off\")\n",
    "        plt.ion()\n",
    "        img = ax.imshow(domain.render(mode=\"rgb_array\"))\n",
    "        display(fig)\n",
    "\n",
    "    # loop until max_steps or goal is reached\n",
    "    for i_step in range(1, max_steps + 1):\n",
    "        if pause_between_steps is not None:\n",
    "            sleep(pause_between_steps)\n",
    "\n",
    "        # choose action according to solver\n",
    "        action = solver.sample_action(observation)\n",
    "        # get corresponding action\n",
    "        outcome = domain.step(action)\n",
    "        observation = outcome.observation\n",
    "\n",
    "        # update image\n",
    "        if render:\n",
    "            img.set_data(domain.render())\n",
    "            fig.canvas.draw()\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "        # final state reached?\n",
    "        if outcome.termination:\n",
    "            break\n",
    "\n",
    "    # close the figure to avoid jupyter duplicating the last image\n",
    "    if render:\n",
    "        plt.close(fig)\n",
    "\n",
    "    # goal reached?\n",
    "    is_goal_reached = observation[0] >= 0.45\n",
    "    if is_goal_reached:\n",
    "        print(f\"Goal reached in {i_step} steps!\")\n",
    "    else:\n",
    "        print(f\"Goal not reached after {i_step} steps!\")\n",
    "\n",
    "    return is_goal_reached, i_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a domain for the roll out and close it at the end. If not closing it, an OpenGL popup windows stays open, at least on local Jupyter sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = domain_factory()\n",
    "try:\n",
    "    rollout(\n",
    "        domain=domain,\n",
    "        solver=solver,\n",
    "        max_steps=300,\n",
    "        pause_between_steps=None,\n",
    "        render=True,\n",
    "    )\n",
    "finally:\n",
    "    domain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some solvers need proper cleaning before being deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver._cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is automatically done if you use the solver within a `with` statement. The syntax would look something like:\n",
    "\n",
    "```python\n",
    "with solver_factory() as solver:\n",
    "    MyDomain.solve_with(solver, domain_factory)\n",
    "    rollout(domain=domain, solver=solver)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve with Reinforcement Learning (RLLib + DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of demonstration, we now show how to solve the same Cart Pole benchmark with a different RL library and algorithm: the [DQN](https://arxiv.org/abs/1312.5602v1) algorithm as implemented in the [Ray RLLib](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#dqn) library.\n",
    "\n",
    "Interestingly, one can note that the domain definition is exactly the same as before. We just change the solving library and algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RayRLlib.check_domain(domain):\n",
    "    solver_factory = lambda: RayRLlib(DQN, train_iterations=100, config=DQNConfig().training(n_step=300))\n",
    "\n",
    "    with solver_factory() as solver:\n",
    "        # Solve domain\n",
    "        GymDomain.solve_with(solver, domain_factory)\n",
    "\n",
    "        # Test solution\n",
    "        rollout(\n",
    "            domain=domain,\n",
    "            solver=solver,\n",
    "            max_steps=300,\n",
    "            pause_between_steps=None,\n",
    "            render=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving planning domains with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning algorithms can learn to solve Markov Decision Processes, a large problem class which also includes path planning problems.\n",
    "\n",
    "To demonstrate this, we consider a single grid world domain, but more challenging path planning problems like mazes can be found in the [documentation](https://airbus.github.io/scikit-decide/notebooks/#maze-tutorial) of the library.\n",
    "\n",
    "We define a domain factory creating a simple grid world environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_factory = lambda: SimpleGridWorld(num_cols=10, num_rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn below a control policy with DQN from RLLib which tries to reach the goal state of the grid located at cell (9, 9) starting from cell (0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RayRLlib.check_domain(domain):\n",
    "    solver_factory = lambda: RayRLlib(\n",
    "        DQN, train_iterations=100, config=DQNConfig().training(n_step=25)\n",
    "    )\n",
    "\n",
    "    with solver_factory() as solver:\n",
    "        # Solve domain\n",
    "        SimpleGridWorld.solve_with(solver, domain_factory)\n",
    "\n",
    "        # Test solution\n",
    "        skd_rollout(\n",
    "            domain=domain,\n",
    "            solver=solver,\n",
    "            max_steps=25,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison, and to show the ability of the scikit-decide library to solve the same domain with algorithms from different research fields, we briefly show how to use the famous [A\\*](https://en.wikipedia.org/wiki/A*_search_algorithm) path planning algorithm to solve the grid domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RayRLlib.check_domain(domain):\n",
    "    solver_factory = lambda: Astar(domain_factory)\n",
    "\n",
    "    with solver_factory() as solver:\n",
    "        # Solve domain\n",
    "        SimpleGridWorld.solve_with(solver, domain_factory)\n",
    "\n",
    "        # Test solution\n",
    "        skd_rollout(\n",
    "            domain=domain,\n",
    "            solver=solver,\n",
    "            max_steps=25,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to control a taxiing aircraft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the demonstration of reinforcement learning with scikit-decide, we want to learn to control the taxiing phase of an aircraft.\n",
    "\n",
    "We rely on the open-source [JSBSim](https://jsbsim.sourceforge.net/) aircraft simulator through the [gym-jsbsim](https://pypi.org/project/gym-jsbsim/) environment.\n",
    "\n",
    "We opt for a continuous learning algorithm named [UCT](https://link.springer.com/chapter/10.1007/11871842_29), a variant of Monte-Carlo Tree Search which became popular to originally solve games like Chess and Go.\n",
    "\n",
    "First, we embed the gym-jsbsim environment in a scikit-decide `GymDomain` compatible with UCT. It requires us to inherit from `GymDomainHashable`, `GymDiscreteActionDomain` and `DeterministicInitialized` which provide methods called by the scikit-decide implementation of the UCT algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"GymJsbsim-TaxiapControlTask-v0\"\n",
    "\n",
    "def normalize_and_round(state):\n",
    "    ns = np.array([s[0] for s in state])\n",
    "    # ns = ns / np.linalg.norm(ns) if np.linalg.norm(ns) != 0 else ns\n",
    "    scale = np.array([10.0, 0.1, 100.0, 100.0, 100.0, 100.0, 1.0, 1.0, 1.0, 1.0])\n",
    "    ns = 1 / (1 + np.exp(-ns / scale))\n",
    "    np.around(ns, decimals=3, out=ns)\n",
    "    return ns\n",
    "\n",
    "\n",
    "class D(GymDomainHashable, GymDiscreteActionDomain, DeterministicInitialized):\n",
    "    pass\n",
    "\n",
    "\n",
    "class GymUCTDomain(D):\n",
    "    \"\"\"This class wraps a cost-based deterministic gymnasium environment as a domain\n",
    "        usable by UCT\n",
    "\n",
    "    !!! warning\n",
    "        Using this class requires gymnasium to be installed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gym_env: gym.Env,\n",
    "        discretization_factor: int = 10,\n",
    "        branching_factor: int = None,\n",
    "        max_depth: int = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize GymUCTDomain.\n",
    "\n",
    "        # Parameters\n",
    "        gym_env: The deterministic Gym environment (gym.env) to wrap.\n",
    "        discretization_factor: Number of discretized action variable values per continuous action variable\n",
    "        branching_factor: if not None, sample branching_factor actions from the resulting list of discretized actions\n",
    "        max_depth: maximum depth of states to explore from the initial state\n",
    "        \"\"\"\n",
    "        GymDomainHashable.__init__(self, gym_env=gym_env)\n",
    "        GymDiscreteActionDomain.__init__(\n",
    "            self,\n",
    "            discretization_factor=discretization_factor,\n",
    "            branching_factor=branching_factor,\n",
    "        )\n",
    "        gym_env._max_episode_steps = max_depth\n",
    "        self._map = None\n",
    "        self._path = None\n",
    "\n",
    "    def _render_from(self, memory: D.T_memory[D.T_state], **kwargs):\n",
    "        # Get rid of the current state and just look at the gym env's current internal state\n",
    "        lon = self._gym_env.sim.get_property_value(prp.position_long_gc_deg)\n",
    "        lat = self._gym_env.sim.get_property_value(prp.position_lat_geod_deg)\n",
    "        if self._map is None:\n",
    "            self._map = folium.Map(location=[lat, lon], zoom_start=18)\n",
    "            taxiPath = taxi_path()\n",
    "            for p in taxiPath.centerlinepoints:\n",
    "                folium.Marker((p[1], p[0]), popup=p).add_to(self._map)\n",
    "            folium.PolyLine(\n",
    "                taxiPath.centerlinepoints, color=\"blue\", weight=2.5, opacity=1\n",
    "            ).add_to(self._map)\n",
    "            self._path = folium.PolyLine(\n",
    "                [(lat, lon)], color=\"red\", weight=2.5, opacity=1\n",
    "            )\n",
    "            self._path.add_to(self._map)\n",
    "            f = open(\"gym_jsbsim_map.html\", \"w\")\n",
    "            f.write(\n",
    "                \"<!DOCTYPE html>\\n\"\n",
    "                + \"<HTML>\\n\"\n",
    "                + \"<HEAD>\\n\"\n",
    "                + '<META http-equiv=\"refresh\" content=\"60\">\\n'\n",
    "                + \"</HEAD>\\n\"\n",
    "                + \"<FRAMESET>\\n\"\n",
    "                + '<FRAME src=\"gym_jsbsim_map_update.html\">\\n'\n",
    "                + \"</FRAMESET>\\n\"\n",
    "                + \"</HTML>\"\n",
    "            )\n",
    "            f.close()\n",
    "        else:\n",
    "            self._path.locations.append(folium.utilities.validate_location((lat, lon)))\n",
    "            self._map.location = folium.utilities.validate_location((lat, lon))\n",
    "        self._map.save(\"gym_jsbsim_map_update.html\")\n",
    "\n",
    "domain_factory = lambda: GymUCTDomain(\n",
    "    gym_env=gym.make(\"GymV21Environment-v0\", env_id=ENV_NAME),\n",
    "    discretization_factor=9,\n",
    "    max_depth=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call UCT with a budget of 10 rollouts or 1 second at each state of the environment. It means that the policy will be optimised at the same time as it is executed: only the first action of the policy is executed in the current state, after the policy has been optimised for maximum 1 second of computation and 10 rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UCT.check_domain(domain_factory()):\n",
    "    solver_factory = lambda: UCT(\n",
    "        domain_factory=domain_factory,\n",
    "        time_budget=1000,\n",
    "        rollout_budget=10,\n",
    "        max_depth=500,\n",
    "        discount=1.0,\n",
    "        transition_mode=UCT.Options.TransitionMode.Step,\n",
    "        continuous_planning=True,\n",
    "        online_node_garbage=True,\n",
    "        parallel=False,\n",
    "        debug_logs=False,\n",
    "    )\n",
    "    with solver_factory() as solver:\n",
    "        GymUCTDomain.solve_with(solver, domain_factory)\n",
    "        solver._domain.reset()\n",
    "        rollout(\n",
    "            domain_factory(),\n",
    "            solver,\n",
    "            num_episodes=1,\n",
    "            max_steps=500,\n",
    "            max_framerate=30,\n",
    "            outcome_formatter=lambda o: f\"{o.observation} - reward: {o.value.reward:.2f}\",\n",
    "            action_formatter=lambda a: f\"{a}\",\n",
    "            verbose=True,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
