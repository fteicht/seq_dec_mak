{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial sequential games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash equilibrium, Pareto-efficiency, zero-sum games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a standard 2-player problem known as the [prisoner's dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) where two rational agents have to decide at the same time, i.e. without knowing the other's decision, whether they should cooperate or not. Their individual rewards depend on each other's decision, assuming each agent knows the other agent's reward model.\n",
    "\n",
    "_<< The police offer each prisoner a Faustian bargain. If he testifies against his partner, he will go free while the partner will get three years in prison on the main charge. Oh, yes, there is a catch ... If both prisoners testify against each other, both will be sentenced to two years in jail. >>_\n",
    "\n",
    "Here is the cost matrix, expressed as pairs of years in prison for each prisoner, depending on each prisoner's decision:\n",
    "\n",
    "| | Prisoner B stays silent | Prisoner B testifies |\n",
    "| --- | --- | --- |\n",
    "| __Prisoner A stays silent__ | Each serves 1 year | Prisoner A: 3 years; Prisoner B: goes free |\n",
    "| __Prisoner A stays A testifies__ | Prisoner A: goes free; Prisoner B: 3 years | Each serves 2 years |\n",
    "\n",
    "![Prisoners dilemma](prisoners_dilemma.png)\n",
    "\n",
    "Each prisoner wants to minimize the worst cost they could get, resulting here in each prisoner testifying against the other.\n",
    "\n",
    "Therefore, each prisoner will serve 2 years in prison whereas they could have served 1 year each if they had cooperated.\n",
    "\n",
    "### Nash Equilibrium\n",
    "\n",
    "This solution is known as the __Nash equilibrium__ of the game which is defined for any non-cooperative game involving 2 or more players.\n",
    "\n",
    "For a given player $i$, let's define $u_i(s_i,s^*_{-i})$ as the utility of the player where $s^*_{-i}$ denotes all the other players' strategies.\n",
    "\n",
    "The _strategy profile_ $s^* = (s^*_i,s^*_{-i})$ is a _Nash equilibrium_ if: $u_i(s^*_i,s^*_{-i}) \\geqslant u_i(s_i,s^*_{-i})$ for all $s_i \\neq s^*_i$.\n",
    "\n",
    "Said differently, a Nash equilibrium is a situation where no player can increase its own expected payoff by changing its strategy while the other players keep theirs unchanged.\n",
    "\n",
    "A game can have zero or many _pure_ Nash equilibria, i.e. where each player deterministically makes its decision. However, each _finite_ game has a single _mixed_ Nash equilibrium, i.e. where each player probabilistically makes its decision.\n",
    "\n",
    "In _zero-sum_ finite games, i.e. where the sum of players' payoffs equals $0$, there exists one (potentially mixed) Nash equilibrium which is equal to the _maximin_ decision: $u_i(s^*_i,s^*_{-i}) = \\max_{s_i} \\min_{s_{-i}} u_i(s_i,s_{-i})$.\n",
    "\n",
    "### Pareto efficiency\n",
    "\n",
    "__Pareto efficiency__ or __Pareto optimality__ is a situation where no player can increase its own expected payoff by changing its strategy without making another player's worse off.\n",
    "\n",
    "In mathematical terms, it means that a $n$-agent strategy $(s_1,\\cdots,s_n)$ is _Pareto-optimal_ if there is no other strategy $(s'_1,\\cdots,s'_n)$ such that $u_i(s'_i,s'_{-i}) \\geqslant u_i(s_i,s_{-i})$ for all $i$ and $u_i(s'_i,s'_{-i}) > u_i(s_i,s_{-i})$ for some $i$.\n",
    "\n",
    "It is worth noting that Pareto-efficient solutions and Nash-equilibria do not need to coincide. While Pareto-optimality is rather sought when players cooperate, Nash-equilibrium concerns non-cooperative players who want to _individually_ maximize their payoffs by considering other players' potential actions. In the case of _zero-sum_ games, Nash equilibria correspond to non-cooperative solutions where each player does not only individually maximize their payoff, but they also want to minimize the potential damages imposed by the adversaries as _adversarial_ responses to their decisions (since some players cannot win without having some other players to lose).\n",
    "\n",
    "![Pareto-optimality vs Nash equilibrium](pareto_optimality_vs_nash_equilibrium.png)\n",
    "\n",
    "### Sequential games\n",
    "\n",
    "A sequential game is a game where a set of players are taking decisions during many successive steps.\n",
    "\n",
    "In a [_repeated game_](https://en.wikipedia.org/wiki/Repeated_game), such as the famous _rock-paper-scissors game_, the players always play the same game in sequence, meaning that the game has only state which is repeated in sequence.\n",
    "\n",
    "In a [_combinatorial game_](https://en.wikipedia.org/wiki/Combinatorial_game_theory), such as the famous _tic-tac-toe game_, the state of the game is fully observable to all the agents and it can potentially change at each step.\n",
    "\n",
    "In the remaining of this course, we will focus on _2-player combinatorial zero-sum games_, where:\n",
    "- the game has many states that can change at each step ;\n",
    "- the 2 players make decisions at each time step until reaching a terminal state of the game or after exhausting the horizon budget ;\n",
    "- the 2 players either win or lose the game at the end of the game.\n",
    "We will actually search for a sequence of [subgames' perfect equilibria](https://en.wikipedia.org/wiki/Subgame_perfect_equilibrium) by using backward induction.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The following assumes perfect information and rational agents. If one of those two conditions are not met, the algorithms below won't necessarily produce winning strategies. We don't play the same strategies in chess in front of a master or a beginner player.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from typing import Any, NamedTuple, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "TIC_TAC_TOE = \"\"\"\n",
    " | | \n",
    "-----\n",
    " | | \n",
    "-----\n",
    " | | \n",
    "\"\"\"\n",
    "\n",
    "class State:\n",
    "    def __init__(self, array: ArrayLike):\n",
    "        self._array = array\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self._array.astype(int).flatten()))\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return np.all(np.equal(self._array, other._array))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self._array)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self._array)\n",
    "    \n",
    "    @property\n",
    "    def array(self):\n",
    "        return self._array\n",
    "    \n",
    "    def copy(self):\n",
    "        return State(self._array.copy())\n",
    "    \n",
    "class Player(Enum):\n",
    "    CrossPlayer = 1\n",
    "    CirclePlayer = 2\n",
    "    \n",
    "class Action(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, tic_tac_toe_str: str = TIC_TAC_TOE):\n",
    "        tic_tac_toe = []\n",
    "        for y, line in enumerate(tic_tac_toe_str.split(\"\\n\")):\n",
    "            row = []\n",
    "            for c in line:\n",
    "                if c == \" \":\n",
    "                    row.append(0)  # spaces are 0s\n",
    "                else:\n",
    "                    row.append(0.7)  # walls are 0.7s\n",
    "            tic_tac_toe.append(row)\n",
    "        self._tic_tac_toe = np.array(tic_tac_toe[1:-1], dtype=np.float32)\n",
    "        self._ax = None\n",
    "        self._fig = None\n",
    "        self._image = None\n",
    "        \n",
    "    def reset(self):\n",
    "        return State(np.zeros(shape=[3, 3], dtype=np.int32))\n",
    "    \n",
    "    def get_next_state(self, state: State, action: Tuple[Player, Action]) -> Tuple[State, float, bool]:\n",
    "        assert state.array[action[1].x][action[1].y] == 0\n",
    "        next_state = state.copy()\n",
    "        next_state.array[action[1].x][action[1].y] = action[0].value\n",
    "        if (np.asarray(np.prod(next_state.array, axis=0) == action[0].value**3).sum() > 0 or\n",
    "            np.asarray(np.prod(next_state.array, axis=1) == action[0].value**3).sum() > 0 or\n",
    "            np.prod(np.diagonal(next_state.array)) == action[0].value**3 or\n",
    "            np.prod(np.diagonal(np.fliplr(next_state.array))) == action[0].value**3):\n",
    "            return next_state, -2 * action[0].value + 3, True\n",
    "        elif np.sum(np.asarray(next_state.array == 0).nonzero()) == 0:\n",
    "            return next_state, 0, True\n",
    "        else:\n",
    "            return next_state, 0, False\n",
    "    \n",
    "    def render(self, state: State) -> Any:\n",
    "        if self._ax is None:\n",
    "            fig, ax = plt.subplots(1)\n",
    "            #fig.canvas.set_window_title(\"tic-tac-toe\")\n",
    "            ax.set_aspect(\"equal\")  # set the x and y axes to the same scale\n",
    "            plt.xticks([])  # remove the tick marks by setting to an empty list\n",
    "            plt.yticks([])  # remove the tick marks by setting to an empty list\n",
    "            ax.invert_yaxis()  # invert the y-axis so the first row of data is at the top\n",
    "            self._ax = ax\n",
    "            self._fig = fig\n",
    "            plt.ion()\n",
    "        if self._image is None:\n",
    "            self._image = self._ax.imshow(self._tic_tac_toe, cmap='Greys', vmin=0, vmax=1)\n",
    "        else:\n",
    "            self._image.set_data(self._tic_tac_toe)\n",
    "        for row in range(3):\n",
    "            for col in range(3):\n",
    "                if state.array[row][col] == 1:\n",
    "                    self._ax.scatter(2 * col, 2 * row, s=500, c='blue', marker='x')\n",
    "                elif state.array[row][col] == 2:\n",
    "                    self._ax.scatter(2 * col, 2 * row, s=500, facecolors='none', edgecolors='red')\n",
    "        display(self._fig)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(1)\n",
    "        \n",
    "tic_tac_toe = TicTacToe()\n",
    "tic_tac_toe.render(tic_tac_toe.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [minimax](https://en.wikipedia.org/wiki/Minimax) algorithm is used to find optimal non-cooperative 2-player strategies where\n",
    "one player will try to maximize its own final score knowing that it will alternate with another player which will\n",
    "try to minimize the first player's score.\n",
    "The algorithm will then alternate maximization and minimization operations, hence its name.\n",
    "Intuitively, the first player will try to maximize the minimum score it can get because of the other player trying\n",
    "to make it lose the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![minimax](./img/Minimax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define the tree structure that will alternate maximizing player nodes and minimizing player node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict\n",
    "from typing import Any, Tuple, List\n",
    "\n",
    "class Tree:\n",
    "    class Node:\n",
    "        def __init__(self,\n",
    "                     data: Any,\n",
    "                     max_player: bool = True,\n",
    "                     terminal: bool = False,\n",
    "                     terminal_value: float = 0,\n",
    "                     best_child: Tuple[Tree.Node, str] = None):\n",
    "            self._data = data\n",
    "            self._max_player = max_player\n",
    "            self._terminal = terminal\n",
    "            self._terminal_value = terminal_value\n",
    "            self._best_child = best_child\n",
    "            self._children: List[Tuple[Tree.Node, str]] = []\n",
    "            \n",
    "        @property\n",
    "        def data(self):\n",
    "            return self._data\n",
    "        \n",
    "        @property\n",
    "        def max_player(self):\n",
    "            return self._max_player\n",
    "        \n",
    "        @property\n",
    "        def terminal(self):\n",
    "            return self._terminal\n",
    "        \n",
    "        @property\n",
    "        def terminal_value(self):\n",
    "            return self._terminal_value\n",
    "        \n",
    "        @property\n",
    "        def best_child(self):\n",
    "            return self._best_child\n",
    "        \n",
    "        def __eq__(self, other: Tree.Node):\n",
    "            return self._data.__eq__(other._data)\n",
    "        \n",
    "        def __hash__(self):\n",
    "            return hash(self._data)\n",
    "        \n",
    "        def __str__(self):\n",
    "            return str(self._data)\n",
    "        \n",
    "        def __repr__(self):\n",
    "            return 'Node(data: {}, max_player: {}, terminal: {}, best child: {})'.format(\n",
    "                repr(self._data),\n",
    "                'true' if self._max_player else 'false',\n",
    "                'true [{}]'.format(self._terminal_value) if self._terminal else 'false',\n",
    "                repr(self._best_child[0]._data) if (self._best_child is not None and self._best_child[0] is not None) else None)\n",
    "            \n",
    "    def __init__(self):\n",
    "        self._nodes: Dict[Any, Tree.Node] = {}\n",
    "    \n",
    "    def get_node(self, data: Any):\n",
    "        if data not in self._nodes:\n",
    "            self._nodes[data] = Tree.Node(data)\n",
    "        return self._nodes[data]\n",
    "        \n",
    "    def get_children(self, node: Node) -> List[Tuple[Node, str]]:\n",
    "        if node.data not in self._nodes or len(self._nodes[node.data]._children) == 0:\n",
    "            if len(node._children) == 0:\n",
    "                node._children = list(self.generate_children(node))\n",
    "            assert all((c[0].max_player and not node.max_player) or\n",
    "                       (not c[0].max_player and node.max_player)\n",
    "                       for c in node._children)\n",
    "            for c in node._children:\n",
    "                self._nodes[c[0].data] = c[0]\n",
    "            self._nodes[node.data] = node\n",
    "        return self._nodes[node.data]._children\n",
    "    \n",
    "    def generate_children(self, node: Node) -> List[Tuple[Node, str]]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def is_terminal(self, node: Node) -> bool:\n",
    "        return node.terminal\n",
    "    \n",
    "    def render(self, node: Node) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the code of the minimax algorithm, as described [there](https://en.wikipedia.org/wiki/Minimax#Pseudocode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def minimax(node : Tree.Node,\n",
    "            tree: Tree,\n",
    "            depth : int,\n",
    "            maximizing_player : bool,\n",
    "            evaluate : Callable[[Tree.Node], float]):\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/minimax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeTree(Tree):\n",
    "    def __init__(self, tic_tac_toe):\n",
    "        super().__init__()\n",
    "        self._tic_tac_toe = tic_tac_toe\n",
    "    \n",
    "    def generate_children(self, node: Tree.Node) -> List[Tuple[Tree.Node, str]]:\n",
    "        state = node.data\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "    \n",
    "    def render(self, node: Tree.Node) -> None:\n",
    "        self._tic_tac_toe.render(node.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/tic_tac_toe_tree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_tac_toe = TicTacToe(TIC_TAC_TOE)\n",
    "tic_tac_toe_tree = TicTacToeTree(tic_tac_toe)\n",
    "\n",
    "minimax(node=tic_tac_toe_tree.get_node(data=tic_tac_toe.reset()),\n",
    "        tree = tic_tac_toe_tree,\n",
    "        depth=1000,\n",
    "        maximizing_player=True,\n",
    "        evaluate = lambda n : n.terminal_value)\n",
    "\n",
    "node = tic_tac_toe_tree.get_node(data=tic_tac_toe.reset())\n",
    "tic_tac_toe.render(node.data)\n",
    "\n",
    "while not node.terminal:\n",
    "    # print('Action: {}'.format(node.best_child[1]))\n",
    "    node = node.best_child[0]\n",
    "    tic_tac_toe.render(node.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Terminal nodes: {len(set(n for n in tic_tac_toe_tree._nodes.values() if n.terminal))}')\n",
    "print(f'Explored nodes: {len(tic_tac_toe_tree._nodes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-Beta Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Alpha-Beta Pruning](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning) algorithm is a popular optimal algorithm\n",
    "to solve non-cooperative 2-player sequential games that improves over minimax by cutting search branches that are known to\n",
    "be useless for one's player strategy to be improved.\n",
    "To do so, the algorithm maintains maximum (_aka_ $\\alpha$) and minimum (_aka_ $\\beta$) values on, respectively, the maximizing\n",
    "player's terminal score and the minimizing player's terminal score.\n",
    "Those bounds allow the algorithm to stop exploring one player's node's subtrees (i.e. pruning the corresponding branches)\n",
    "whenever the already discovered subtrees are sufficient to prove that the other player will necessarily choose a different option\n",
    "than the one leading to the first player's node's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alpha-Beta Pruning](img/AB_pruning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudocode of the Alpha-Beta Pruning algorithm is described [there](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning#Pseudocode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def alphabeta(node : Tree.Node,\n",
    "              tree: Tree,\n",
    "              depth : int,\n",
    "              alpha : float,\n",
    "              beta : float,\n",
    "              maximizing_player : bool,\n",
    "              evaluate : Callable[[Tree.Node], float]):\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/alphabeta.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_tac_toe = TicTacToe(TIC_TAC_TOE)\n",
    "tic_tac_toe_tree = TicTacToeTree(tic_tac_toe)\n",
    "\n",
    "alphabeta(node=tic_tac_toe_tree.get_node(data=tic_tac_toe.reset()),\n",
    "          tree = tic_tac_toe_tree,\n",
    "          depth=1000,\n",
    "          alpha=-float(\"inf\"),\n",
    "          beta=float(\"inf\"),\n",
    "          maximizing_player=True,\n",
    "          evaluate = lambda n : n.terminal_value)\n",
    "\n",
    "node = tic_tac_toe_tree.get_node(data=tic_tac_toe.reset())\n",
    "tic_tac_toe.render(node.data)\n",
    "\n",
    "while not node.terminal:\n",
    "    # print('Action: {}'.format(node.best_child[1]))\n",
    "    node = node.best_child[0]\n",
    "    tic_tac_toe.render(node.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Terminal nodes: {len(set(n for n in tic_tac_toe_tree._nodes.values() if n.terminal))}')\n",
    "print(f'Explored nodes: {len(tic_tac_toe_tree._nodes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing against a random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "tic_tac_toe = TicTacToe(TIC_TAC_TOE)\n",
    "tic_tac_toe_tree = TicTacToeTree(tic_tac_toe)\n",
    "\n",
    "def call_alphabeta_pruning(tic_tac_toe_tree: TicTacToeTree,\n",
    "                           node: Tree.Node) -> None:\n",
    "    alphabeta(node=node,\n",
    "              tree = tic_tac_toe_tree,\n",
    "              depth=1000,\n",
    "              alpha=-float(\"inf\"),\n",
    "              beta=float(\"inf\"),\n",
    "              maximizing_player=True,\n",
    "              evaluate = lambda n : n.terminal_value)\n",
    "    \n",
    "def call_random_player(tic_tac_toe_tree: TicTacToeTree,\n",
    "                       node: Tree.Node) -> None:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "    pass\n",
    "\n",
    "node = tic_tac_toe_tree.get_node(data=tic_tac_toe.reset())\n",
    "tic_tac_toe.render(node.data)\n",
    "\n",
    "while not node.terminal:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "        \n",
    "    node = node.best_child[0]\n",
    "    tic_tac_toe.render(node.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/alphabeta_vs_random.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing _optimally_ against a random player"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the opponent is playing randomly, we can exploit the knowledge of its policy to optimize our own strategy.\n",
    "For this, we can model our own process choice as a Markov Decision Process and solve the game _from our perspective_ by using\n",
    "an MDP algorithm like RTDP.\n",
    "We will use [scikit-decide](https://github.com/airbus/scikit-decide) to model and solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "from skdecide.hub.space.gym import ListSpace\n",
    "\n",
    "class D(MDPDomain, Goals, Renderable):\n",
    "    T_state = Tree.Node  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Tuple[Tree.Node, str]  # Type of events/actions\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class ProbabilisticGameDomain(D):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 game_tree: Tree,\n",
    "                 root_node: Tree.Node,\n",
    "                 opponent_policy: Callable[[Tree.Node],\n",
    "                                           List[Tuple[float, Tree.Node]]],\n",
    "                 max_or_min_player: bool = True):\n",
    "        self._game_tree = game_tree\n",
    "        self._game_root_node = root_node\n",
    "        self._opponent_policy = opponent_policy\n",
    "        self._max_or_min_player = max_or_min_player\n",
    "    \n",
    "    def _is_terminal(self, state: D.T_state) -> D.T_predicate:\n",
    "        return self._game_tree.is_terminal(state)\n",
    "    \n",
    "    def _get_transition_value(self, memory: D.T_state, action: D.T_event, next_state: Optional[D.T_state] = None) -> Value[D.T_value]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "        pass\n",
    "    \n",
    "    def _get_next_state_distribution(self, memory: D.T_state, action: D.T_event) -> DiscreteDistribution[D.T_state]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _get_applicable_actions_from(self, memory: D.T_state) -> Space[D.T_event]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "        pass\n",
    "    \n",
    "    def _get_goals_(self) -> Space[D.T_observation]:\n",
    "        return ImplicitSpace(lambda s: self._game_tree.is_terminal(s))\n",
    "    \n",
    "    def _get_initial_state_(self) -> D.T_state:\n",
    "        return self._game_root_node\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        self._game_tree.render(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/probabilistic_game_domain.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use LRTDP from `scikit-decide` as our MDP solver, as described in Algorthm 1 in [this paper](https://ftp.cs.ucla.edu/pub/stat_ser/R319.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from skdecide.hub.solver.lrtdp import LRTDP\n",
    "\n",
    "tic_tac_toe = TicTacToe(TIC_TAC_TOE)\n",
    "tic_tac_toe_tree = TicTacToeTree(tic_tac_toe)\n",
    "\n",
    "def call_game_rtdp(tree: Tree,\n",
    "                   node: Tree.Node,\n",
    "                   opponent_policy: Callable[[Tree.Node],\n",
    "                                             List[Tuple[float, Tree.Node]]],\n",
    "                   max_value: float) -> None:\n",
    "    domain_factory = lambda: ProbabilisticGameDomain(tree, node, opponent_policy)\n",
    "    rtdp_factory = lambda: LRTDP(\n",
    "        domain_factory=domain_factory,\n",
    "        heuristic = lambda d, s : Value(reward=max_value),\n",
    "        discount=1.0,\n",
    "        epsilon=0.001,\n",
    "        parallel=False,\n",
    "        debug_logs=False)\n",
    "    with rtdp_factory() as rtdp:\n",
    "        ProbabilisticGameDomain.solve_with(rtdp, domain_factory)\n",
    "        node._best_child = rtdp.sample_action(node)\n",
    "    \n",
    "def call_random_player(tic_tac_toe_tree: TicTacToeTree,\n",
    "                       node: Tree.Node) -> None:\n",
    "    node._best_child = random.sample(tic_tac_toe_tree.get_children(node), 1)[0]\n",
    "    \n",
    "def random_player_policy(node: Tree.Node) -> List[Tuple[float, Tree.Node]]:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "    pass\n",
    "\n",
    "node = tic_tac_toe_tree.get_node(data=tic_tac_toe.reset())\n",
    "tic_tac_toe.render(node.data)\n",
    "\n",
    "while not node.terminal:\n",
    "    if node.max_player:\n",
    "        call_game_rtdp(tic_tac_toe_tree, node, random_player_policy, 1)\n",
    "    else:\n",
    "        call_random_player(tic_tac_toe_tree, node)\n",
    "        \n",
    "    node = node.best_child[0]\n",
    "    tic_tac_toe.render(node.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rtdp_vs_random.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to epic combats: RTDP vs RTDP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_tac_toe = TicTacToe(TIC_TAC_TOE)\n",
    "tic_tac_toe_tree = TicTacToeTree(tic_tac_toe)\n",
    "\n",
    "def opponent_policy(tree_node: Tree.Node,\n",
    "                    opponent_pol: Dict[Tree.Node,\n",
    "                                       Tuple[Tuple[Tree.Node, str], float]]) -> List[Tuple[float, Tree.Node]]:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "    pass\n",
    "\n",
    "def call_game_rtdp(tree: Tree,\n",
    "                   node: Tree.Node,\n",
    "                   opponent_pol: Dict[Tree.Node,\n",
    "                                      Tuple[Tuple[Tree.Node, str], float]],\n",
    "                   max_value: float,\n",
    "                   max_or_min_player: bool) -> None:\n",
    "    domain_factory = lambda: ProbabilisticGameDomain(tree,\n",
    "                                                     node,\n",
    "                                                     lambda tree_node: opponent_policy(tree_node, opponent_pol),\n",
    "                                                     max_or_min_player)\n",
    "    rtdp_factory = lambda: LRTDP(\n",
    "        domain_factory=domain_factory,\n",
    "        heuristic = lambda d, s : Value(reward=max_value) if max_or_min_player else Value(cost=max_value),\n",
    "        discount=1.0,\n",
    "        epsilon=0.001,\n",
    "        parallel=False,\n",
    "        debug_logs=False)\n",
    "    with rtdp_factory() as rtdp:\n",
    "        ProbabilisticGameDomain.solve_with(rtdp, domain_factory)\n",
    "        node._best_child = rtdp.sample_action(node)\n",
    "        policy = rtdp.get_policy()\n",
    "    \n",
    "    return policy\n",
    "\n",
    "node = tic_tac_toe_tree.get_node(data=tic_tac_toe.reset())\n",
    "tic_tac_toe.render(node.data)\n",
    "current_opponent_policy = None\n",
    "\n",
    "while not node.terminal:\n",
    "    print('Player {}\\'s turn'.format(\n",
    "        'Cross' if node.max_player else 'Circle'\n",
    "    ))\n",
    "    current_opponent_policy = call_game_rtdp(\n",
    "        tic_tac_toe_tree,\n",
    "        node,\n",
    "        current_opponent_policy,\n",
    "        1,\n",
    "        node.max_player\n",
    "    )\n",
    "    \n",
    "    node = node.best_child[0]\n",
    "    tic_tac_toe.render(node.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rtdp_vs_rtdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Sequential game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider the case of stochastic games, where the effects of each player's actions are noised after each time step.\n",
    "\n",
    "Let's first implement a probabilistic tree which can model this behaviour during the search for the best strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class ProbabilisticTree:\n",
    "    class StateNode:\n",
    "        def __init__(\n",
    "            self,\n",
    "            data: Any,\n",
    "            max_player: bool = True,\n",
    "            terminal: bool = False,\n",
    "            terminal_value: float = 0,\n",
    "            best_child: Tuple[ProbabilisticTree.ActionNode, str] = None,\n",
    "        ):\n",
    "            self._data = data\n",
    "            self._max_player = max_player\n",
    "            self._terminal = terminal\n",
    "            self._terminal_value = terminal_value\n",
    "            self._best_child = best_child\n",
    "            self._children: List[Tuple[ProbabilisticTree.ActionNode, str]] = []\n",
    "\n",
    "        @property\n",
    "        def data(self):\n",
    "            return self._data\n",
    "\n",
    "        @property\n",
    "        def max_player(self):\n",
    "            return self._max_player\n",
    "\n",
    "        @property\n",
    "        def terminal(self):\n",
    "            return self._terminal\n",
    "\n",
    "        @property\n",
    "        def terminal_value(self):\n",
    "            return self._terminal_value\n",
    "\n",
    "        @property\n",
    "        def best_child(self):\n",
    "            return self._best_child\n",
    "\n",
    "        def __eq__(self, other: Tree.StateNode):\n",
    "            return self._data.__eq__(other._data)\n",
    "\n",
    "        def __hash__(self):\n",
    "            return hash(self._data)\n",
    "\n",
    "        def __str__(self):\n",
    "            return str(self._data)\n",
    "\n",
    "        def __repr__(self):\n",
    "            return (\n",
    "                \"Node(data: {}, max_player: {}, terminal: {}, best child: {})\".format(\n",
    "                    repr(self._data),\n",
    "                    \"true\" if self._max_player else \"false\",\n",
    "                    (\n",
    "                        \"true [{}]\".format(self._terminal_value)\n",
    "                        if self._terminal\n",
    "                        else \"false\"\n",
    "                    ),\n",
    "                    (\n",
    "                        repr(self._best_child[0]._data)\n",
    "                        if (\n",
    "                            self._best_child is not None\n",
    "                            and self._best_child[0] is not None\n",
    "                        )\n",
    "                        else None\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    class ActionNode:\n",
    "        def __init__(self, data: Any) -> None:\n",
    "            self._children: List[\n",
    "                Tuple[ProbabilisticTree.StateNode, str, float]\n",
    "            ] = []\n",
    "\n",
    "        @property\n",
    "        def data(self):\n",
    "            return self._data\n",
    "\n",
    "        def __eq__(self, other: ProbabilisticTree.ActionNode):\n",
    "            return self._data.__eq__(other._data)\n",
    "\n",
    "        def __hash__(self):\n",
    "            return hash(self._data)\n",
    "\n",
    "        def __str__(self):\n",
    "            return str(self._data)\n",
    "\n",
    "        def __repr__(self):\n",
    "            return \"Node(data: {})\".format(repr(self._data))\n",
    "\n",
    "    def __init__(self):\n",
    "        self._nodes: Dict[Any, ProbabilisticTree.StateNode] = {}\n",
    "\n",
    "    def get_node(self, data: Any):\n",
    "        if data not in self._nodes:\n",
    "            self._nodes[data] = ProbabilisticTree.StateNode(data)\n",
    "        return self._nodes[data]\n",
    "\n",
    "    def get_children(\n",
    "        self, node: ProbabilisticTree.StateNode\n",
    "    ) -> List[Tuple[ProbabilisticTree.ActionNode, str]]:\n",
    "        if node.data not in self._nodes or len(self._nodes[node.data]._children) == 0:\n",
    "            if len(node._children) == 0:\n",
    "                node._children = list(self.generate_children(node))\n",
    "            assert all(\n",
    "                (cc[0].max_player and not node.max_player)\n",
    "                or (not cc[0].max_player and node.max_player)\n",
    "                for c in node._children for cc in c[0]._children\n",
    "            )\n",
    "            for c in node._children:\n",
    "                for cc in c[0]._children:\n",
    "                    self._nodes[cc[0].data] = cc[0]\n",
    "            self._nodes[node.data] = node\n",
    "        return self._nodes[node.data]._children\n",
    "\n",
    "    def generate_children(\n",
    "        self, node: ProbabilisticTree.StateNode\n",
    "    ) -> List[Tuple[ProbabilisticTree.ActionNode, str]]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def is_terminal(self, node: ProbabilisticTree.StateNode) -> bool:\n",
    "        return node.terminal\n",
    "\n",
    "    def render(self, node: ProbabilisticTree.StateNode) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a probabilistic version of the tic-tac-toe game where the position $(x, y)$ chosen by a player at a given point in time can be randomly changed to the mirror position $(y, x)$ if the latter is free in the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticTicTacToeTree(ProbabilisticTree):\n",
    "    def __init__(self, tic_tac_toe):\n",
    "        super().__init__()\n",
    "        self._tic_tac_toe = tic_tac_toe\n",
    "\n",
    "    def generate_children(self, node: ProbabilisticTree.StateNode) -> List[Tuple[ProbabilisticTree.ActionNode, str]]:\n",
    "        state = node.data\n",
    "        avail_posx, avail_posy = np.asarray(state.array == 0).nonzero()\n",
    "        for i in range(len(avail_posx)):\n",
    "            succs = [(avail_posx[i], avail_posy[i])]\n",
    "            if (avail_posy[i], avail_posx[i]) in [(avail_posx[ii], avail_posy[ii]) for ii in range(len(avail_posx))]:\n",
    "                succs.append((avail_posy[i], avail_posx[i]))\n",
    "            nsuccs = []\n",
    "            for s in succs:\n",
    "                next_state, value, terminal = self._tic_tac_toe.get_next_state(\n",
    "                    state,\n",
    "                    (\n",
    "                        (\n",
    "                            Player.CrossPlayer\n",
    "                            if node.max_player\n",
    "                            else Player.CirclePlayer\n",
    "                        ),\n",
    "                        Action(\n",
    "                            x=s[0],\n",
    "                            y=s[1],\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "                nsuccs.append((next_state, value, terminal, s[0], s[1]))\n",
    "            action_node = ProbabilisticTree.ActionNode(data=nsuccs[0][0])\n",
    "            action_node._children = [\n",
    "                (\n",
    "                    ProbabilisticTree.StateNode(\n",
    "                        data=s[0],\n",
    "                        max_player=not node.max_player,\n",
    "                        terminal=s[2],\n",
    "                        terminal_value=s[1],\n",
    "                    ),\n",
    "                    \"{} at ({}, {})\".format(\n",
    "                        \"cross\" if node.max_player else \"circle\",\n",
    "                        str(s[3]),\n",
    "                        str(s[4]),\n",
    "                    ),\n",
    "                    1. / len(nsuccs)\n",
    "                )\n",
    "                for s in nsuccs\n",
    "            ]\n",
    "            yield (\n",
    "                action_node,\n",
    "                \"{} at ({}, {})\".format(\n",
    "                    \"cross\" if node.max_player else \"circle\",\n",
    "                    str(avail_posx[i]),\n",
    "                    str(avail_posy[i]),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def render(self, node: Tree.Node) -> None:\n",
    "        self._tic_tac_toe.render(node.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of zero-sum games with stochastic action effects can be solved by a probabilistic extension of the _minimax_ algorithm named [_expectiminimax_](https://en.wikipedia.org/wiki/Expectiminimax). In the expectiminimax algorithm presented in Wikipedia, actions do no need to be all stochastic. We propose below a version of the algorithm which is simpler to implement while being more general, where we assume that each action effect is stochastic (yet potentially with a single outcome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def expectiminimax(\n",
    "    node: ProbabilisticTree.StateNode,\n",
    "    tree: ProbabilisticTree,\n",
    "    depth: int,\n",
    "    maximizing_player: bool,\n",
    "    evaluate: Callable[[ProbabilisticTree.StateNode], float],\n",
    "):\n",
    "    if depth == 0 or tree.is_terminal(node):\n",
    "        return evaluate(node)\n",
    "    if maximizing_player:\n",
    "        value = -float(\"inf\")\n",
    "        for child in tree.get_children(node):\n",
    "            ### WRITE YOUR CODE HERE\n",
    "            # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "            if tentative >= value:\n",
    "                node._best_child = child\n",
    "                value = tentative\n",
    "        return value\n",
    "    else:\n",
    "        value = float(\"inf\")\n",
    "        for child in tree.get_children(node):\n",
    "            ### WRITE YOUR CODE HERE\n",
    "            # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "            if tentative <= value:\n",
    "                node._best_child = child\n",
    "                value = tentative\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/expectiminimax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_tac_toe = TicTacToe(TIC_TAC_TOE)\n",
    "tic_tac_toe_tree = ProbabilisticTicTacToeTree(tic_tac_toe)\n",
    "\n",
    "node = tic_tac_toe_tree.get_node(data=tic_tac_toe.reset())\n",
    "tic_tac_toe.render(node.data)\n",
    "\n",
    "while not node.terminal:\n",
    "\n",
    "    if node.best_child is None:\n",
    "        expectiminimax(node=node,\n",
    "              tree = ProbabilisticTicTacToeTree(tic_tac_toe),\n",
    "              depth=3,\n",
    "              maximizing_player=True,\n",
    "              evaluate = lambda n : n.terminal_value\n",
    "        )\n",
    "    \n",
    "    action_node = node.best_child[0]\n",
    "    next_node = random.choices(\n",
    "        population=[cc[0] for cc in action_node._children],\n",
    "        weights=[cc[2] for cc in action_node._children],\n",
    "        k=1,\n",
    "    )[0]\n",
    "    tic_tac_toe.render(next_node.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a more difficult game: connect-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muzero.games.connect4 import Connect4\n",
    "\n",
    "connect4 = Connect4()\n",
    "connect4.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class Board:\n",
    "    def __init__(self, board: ArrayLike):\n",
    "        self._board = board\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self._board.astype(int).flatten()))\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return np.all(np.equal(self._board, other._board))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self._board)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self._board)\n",
    "    \n",
    "    @property\n",
    "    def board(self):\n",
    "        return self._board\n",
    "    \n",
    "    def copy(self):\n",
    "        return Board(self._board.copy())\n",
    "    \n",
    "\n",
    "class Connect4Tree(Tree):\n",
    "    # We will store the Connect4 boards in the tree nodes\n",
    "    # I.e: Tree.Node._data is a Board (i.e. hashable numpy array) as defined in the Connect4 class\n",
    "            \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._ax = None\n",
    "        self._fig = None\n",
    "        self._image = None\n",
    "    \n",
    "    def reset(self) -> Tree.Node:\n",
    "        connect4 = Connect4()\n",
    "        connect4.reset()\n",
    "        return self.get_node(data=Board(connect4.board))\n",
    "    \n",
    "    def generate_children(self, node: Tree.Node) -> List[Tuple[Tree.Node, str]]:\n",
    "        connect4 = Connect4()\n",
    "        connect4.board = node.data.board\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        # If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "    \n",
    "    def render(self, node: Tree.Node) -> None:\n",
    "        board_to_render = np.zeros(shape=(2 * node.data.board[::-1].shape[0] + 1,\n",
    "                                          2 * node.data.board[::-1].shape[1] + 1),\n",
    "                                   dtype=np.float32)\n",
    "        \n",
    "        for r in range(int(board_to_render.shape[0] / 2) + 1):\n",
    "            board_to_render[2*r,:] = 0.7 * np.ones(board_to_render.shape[1])\n",
    "        for c in range(int(board_to_render.shape[1] / 2) + 1):\n",
    "            board_to_render[:,2*c] = 0.7 * np.ones(board_to_render.shape[0])\n",
    "            \n",
    "        if self._ax is None:\n",
    "            fig, ax = plt.subplots(1)\n",
    "            fig.canvas.set_window_title(\"connect-4\")\n",
    "            ax.set_aspect(\"equal\")  # set the x and y axes to the same scale\n",
    "            plt.xticks([])  # remove the tick marks by setting to an empty list\n",
    "            plt.yticks([])  # remove the tick marks by setting to an empty list\n",
    "            ax.invert_yaxis()  # invert the y-axis so the first row of data is at the top\n",
    "            self._ax = ax\n",
    "            self._fig = fig\n",
    "            plt.ion()\n",
    "        if self._image is None:\n",
    "            self._image = self._ax.imshow(board_to_render, cmap='Greys', vmin=0, vmax=1)\n",
    "        else:\n",
    "            self._image.set_data(board_to_render)\n",
    "        \n",
    "        for r in range(node.data.board[::-1].shape[0]):\n",
    "            for c in range(node.data.board[::-1].shape[1]):\n",
    "                if node.data.board[::-1][r,c] == 1:\n",
    "                    self._ax.scatter(2*c + 1, 2*r + 1, facecolors='green', edgecolors='green')\n",
    "                elif node.data.board[::-1][r,c] == -1:\n",
    "                    self._ax.scatter(2*c + 1, 2*r + 1, facecolors='red', edgecolors='red')\n",
    "        \n",
    "        display(self._fig)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(1)\n",
    "\n",
    "connect4_tree = Connect4Tree()\n",
    "connect4_tree.render(connect4_tree.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/connect4_tree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect4_tree = Connect4Tree()\n",
    "\n",
    "node = connect4_tree.reset()\n",
    "connect4_tree.render(node)\n",
    "\n",
    "while not node.terminal:\n",
    "    \n",
    "    if node.best_child is None:\n",
    "        connect4_tree = Connect4Tree()\n",
    "        alphabeta(node=node,\n",
    "              tree = connect4_tree,\n",
    "              depth=6,\n",
    "              alpha=-float(\"inf\"),\n",
    "              beta=float(\"inf\"),\n",
    "              maximizing_player=True,\n",
    "              evaluate = lambda n : n.terminal_value\n",
    "        )\n",
    "        \n",
    "    print('Action: {}'.format(node.best_child[1]))\n",
    "    node = node.best_child[0]\n",
    "    connect4_tree.render(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MuZero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MuZero](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules) is a popular algorithm for optimizing\n",
    "non-cooperative 2-player sequential games using [Monte-Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)\n",
    "and deep-learning.\n",
    "Like when we tried to play RTDP against itself, MuZero will learn a playing strategy in the form of a deep neural network policy\n",
    "by learning to play against itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MCTS](img/MCTS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MuZero](img/muzero.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd muzero/\n",
    "!pip install -r requirements.txt\n",
    "!pip uninstall -y pyarrow\n",
    "%load_ext tensorboard\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./muzero/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd muzero/\n",
    "!python muzero.py connect4 '{\"training_steps\": 100}'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.path.abspath('')\n",
    "muzero_dir = os.path.join(current_dir, 'muzero')\n",
    "%env PYTHONPATH=$muzero_dir:$current_dir\n",
    "\n",
    "import sys\n",
    "sys.path.append(muzero_dir)\n",
    "sys.path.append(current_dir)\n",
    "\n",
    "from muzero.muzero import MuZero\n",
    "from muzero.games.connect4 import MuZeroConfig, Game\n",
    "\n",
    "config = MuZeroConfig()\n",
    "config.training_steps = 100\n",
    "muzero = MuZero('connect4', config)\n",
    "muzero.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
