{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning course graded exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will consider a probabilistic maze problem defined as a Markov Decision Process [scikit-decide](https://github.com/airbus/scikit-decide) domain, where the controlled agent moving in the maze can magically teleport itself upon some events.\n",
    "You will then solve the problem by using one of the probabilistic algorithms provided by `scikit-decide`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to select the right planning class in `scikit-decide`'s [domain class generator](https://airbus.github.io/scikit-decide/codegen/), i.e. `MPDDomain` which is additionnally `renderable` and have `goals`.\n",
    "It will generate the following domain class template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "\n",
    "\n",
    "# Example of State type (adapt to your needs)\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "# Example of Action type (adapt to your needs)\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class D(MDPDomain, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class MyDomain(D):\n",
    "    \n",
    "    def _is_terminal(self, state: D.T_state) -> D.T_predicate:\n",
    "        pass\n",
    "    \n",
    "    def _get_transition_value(self, memory: D.T_state, action: D.T_event, next_state: Optional[D.T_state] = None) -> Value[D.T_value]:\n",
    "        pass\n",
    "    \n",
    "    def _get_next_state_distribution(self, memory: D.T_state, action: D.T_event) -> SingleValueDistribution[D.T_state]:\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _get_applicable_actions_from(self, memory: D.T_state) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _get_goals_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _get_initial_state_(self) -> D.T_state:\n",
    "        pass\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a `TeleportationMazeDomain` class from this template.\n",
    "We assume that whenever the agent attempts to go up, it will go back to its initial state with probability 0.9.\n",
    "Every move costs 10 but 20 when we hit a wall or 1 when the agent goes through a \"dollar\" light green cell.\n",
    "Therefore, the agent must find a compromise between going up in the middle of the maze to gather the dollars and not being teleported to the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "from copy import deepcopy\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "from skdecide.hub.space.gym import EnumSpace, ListSpace\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "DEFAULT_MAZE = \"\"\"\n",
    "+-+-+-+-+o+-+-+-+-+-+\n",
    "|   |             | |\n",
    "+ + + +-+-+-+ +-+ + +\n",
    "| | |   |   | | |   |\n",
    "+ +-+-+ +-+ + + + +-+\n",
    "| |   |   | |   |   |\n",
    "+ + + + + + + +-+ +-+\n",
    "|   |   |   | |     |\n",
    "+-+-+-+-+-+-+-+ +-+ +\n",
    "|             |   | |\n",
    "+ +-+-+-+-+$$$+-+-+ +\n",
    "|   |    $$$|       |\n",
    "+ + + +-+$+-+ +-+-+-+\n",
    "| | |         |     |\n",
    "+ +-+-+ + +-+ + +-+ +\n",
    "| |     | | | |   | |\n",
    "+-+ +-+ + + + +-+ + +\n",
    "|   |   |   |   | | |\n",
    "+ +-+ +-+-+-+-+ + + +\n",
    "|   |       |     | |\n",
    "+-+-+-+-+-+x+-+-+-+-+\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example of State type (adapt to your needs)\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "# Example of Action type (adapt to your needs)\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class D(MDPDomain, Goals, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class TeleportationMazeDomain(D):\n",
    "    \n",
    "    def __init__(self, maze_str: str = DEFAULT_MAZE, dollar_cost=1.0, dollar_probability=0.9):\n",
    "        maze = []\n",
    "        self._dollars = []\n",
    "        for y, line in enumerate(maze_str.strip().split(\"\\n\")):\n",
    "            line = line.rstrip()\n",
    "            row = []\n",
    "            for x, c in enumerate(line):\n",
    "                if c in {\" \", \"o\", \"x\", \"$\"}:\n",
    "                    row.append(1)  # spaces are 1s\n",
    "                    if c == \"o\":\n",
    "                        self._start = State(x, y)\n",
    "                    if c == \"x\":\n",
    "                        self._goal = State(x, y)\n",
    "                    if c == \"$\":\n",
    "                        self._dollars.append(State(x, y))\n",
    "                else:\n",
    "                    row.append(0)  # walls are 0s\n",
    "            maze.append(row)\n",
    "        # self._render_maze = deepcopy(self._maze)\n",
    "        self._maze = maze\n",
    "        self._num_cols = len(maze[0])\n",
    "        self._num_rows = len(maze)\n",
    "        self._dollar_cost = dollar_cost\n",
    "        self._dollar_probability = dollar_probability\n",
    "        self._ax = None\n",
    "        self._fig = None\n",
    "        self._image = None\n",
    "    \n",
    "    def _is_terminal(self, state: D.T_state) -> D.T_predicate:\n",
    "        return state == self._goal\n",
    "    \n",
    "    def _get_transition_value(self, memory: D.T_state, action: D.T_event, next_state: Optional[D.T_state] = None) -> Value[D.T_value]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def _get_next_state_distribution(self, memory: D.T_state, action: D.T_event) -> DiscreteDistribution[D.T_state]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        return EnumSpace(Action)\n",
    "    \n",
    "    def _get_applicable_actions_from(self, memory: D.T_state) -> Space[D.T_event]:\n",
    "        if memory == self._goal:\n",
    "            return ListSpace([Action.down])\n",
    "        transitions = [\n",
    "            (Action.left, State(memory.x - 1, memory.y)),\n",
    "            (Action.right, State(memory.x + 1, memory.y)),\n",
    "            (Action.up, State(memory.x, memory.y - 1)),\n",
    "            (Action.down, State(memory.x, memory.y + 1))\n",
    "        ]\n",
    "        applicable_actions = []\n",
    "        for tr in transitions:\n",
    "            if (\n",
    "                0 <= tr[1].x < self._num_cols\n",
    "                and 0 <= tr[1].y < self._num_rows\n",
    "                and self._maze[tr[1].y][tr[1].x] == 1\n",
    "            ):\n",
    "                applicable_actions.append(tr[0])\n",
    "        assert len(applicable_actions) > 0\n",
    "        return ListSpace(applicable_actions)\n",
    "    \n",
    "    def _get_goals_(self) -> Space[D.T_observation]:\n",
    "        return ListSpace([self._goal])\n",
    "    \n",
    "    def _get_initial_state_(self) -> D.T_state:\n",
    "        return self._start\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        if self._ax is None:\n",
    "            fig, ax = plt.subplots(1)\n",
    "            #fig.canvas.set_window_title(\"Maze\")\n",
    "            ax.set_aspect(\"equal\")  # set the x and y axes to the same scale\n",
    "            plt.xticks([])  # remove the tick marks by setting to an empty list\n",
    "            plt.yticks([])  # remove the tick marks by setting to an empty list\n",
    "            ax.invert_yaxis()  # invert the y-axis so the first row of data is at the top\n",
    "            self._ax = ax\n",
    "            self._fig = fig\n",
    "            plt.ion()\n",
    "        maze = deepcopy(self._maze)\n",
    "        for d in self._dollars:\n",
    "            maze[d.y][d.x] = 0.9\n",
    "        maze[self._goal.y][self._goal.x] = 0.6\n",
    "        maze[memory.y][memory.x] = 0.3\n",
    "        if 'path' in kwargs:\n",
    "            for s in kwargs.get('path'):\n",
    "                maze[s.y][s.x] = 0.5\n",
    "        if self._image is None:\n",
    "            self._image = self._ax.imshow(maze)\n",
    "        else:\n",
    "            self._image.set_data(maze)\n",
    "        display(self._fig)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(0.001)\n",
    "        \n",
    "maze = TeleportationMazeDomain()\n",
    "maze.render(maze._get_initial_state_())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we solve the domain with the LRTDP solver, which is a famous extension of the LRTA* solver to solve Stochastic Shortest Path problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skdecide.hub.solver.lrtdp import LRTDP\n",
    "from skdecide.utils import rollout\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "domain_factory = lambda: TeleportationMazeDomain(maze_str=DEFAULT_MAZE, dollar_cost=1.0, dollar_probability=0.9)\n",
    "\n",
    "solver_factory = lambda: LRTDP(\n",
    "    domain_factory=domain_factory,\n",
    "    heuristic=lambda d, s: Value(cost=sqrt((d._goal.x - s.x)**2 + (d._goal.y - s.y)**2)),\n",
    "    discount=1.0,\n",
    "    epsilon=0.001,\n",
    "    parallel=False,\n",
    "    debug_logs=False,\n",
    ")\n",
    "\n",
    "with solver_factory() as solver:\n",
    "    TeleportationMazeDomain.solve_with(solver, domain_factory)\n",
    "    \n",
    "    plan = []\n",
    "    cost = 0\n",
    "    evaluation_domain = domain_factory()\n",
    "    observation = evaluation_domain.reset()\n",
    "    path = [observation]\n",
    "    nb_steps = 0\n",
    "    while (not evaluation_domain.is_goal(observation)) and nb_steps < 1000:\n",
    "        plan.append(solver.sample_action(observation))\n",
    "        outcome = evaluation_domain.step(plan[-1])\n",
    "        cost += outcome.value.cost\n",
    "        observation = outcome.observation\n",
    "        path.append(observation)\n",
    "        nb_steps += 1\n",
    "        \n",
    "    print(f'Found a plan of cost {cost}: {plan}')\n",
    "    for i, s in enumerate(path):\n",
    "        evaluation_domain.render(s, path=path[:i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play with different `dollar_cost` and `dollar_probability` until finding a path that goes through the dollars in the maze.\n",
    "Please comment your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
