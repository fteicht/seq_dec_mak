{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning course graded exercice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will solve a probabilistic maze problem defined as a Markov Decision Process ![scikit-decide](https://github.com/airbus/scikit-decide) domain.\n",
    "You will then solve the problem by using one of the probabilistic algorithms provided by `scikit-decide`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to select the right planning class in `sciki-decide`'s ![domain class generator](https://airbus.github.io/scikit-decide/codegen/), i.e. `MPDDomain` which is additionnally `renderable`.\n",
    "It will generate the following domain class template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "\n",
    "\n",
    "# Example of State type (adapt to your needs)\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "# Example of Action type (adapt to your needs)\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class D(MDPDomain, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class MyDomain(D):\n",
    "    \n",
    "    def _is_terminal(self, state: D.T_state) -> D.T_predicate:\n",
    "        pass\n",
    "    \n",
    "    def _get_transition_value(self, memory: D.T_state, action: D.T_event, next_state: Optional[D.T_state] = None) -> Value[D.T_value]:\n",
    "        pass\n",
    "    \n",
    "    def _get_next_state_distribution(self, memory: D.T_state, action: D.T_event) -> SingleValueDistribution[D.T_state]:\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _get_applicable_actions_from(self, memory: D.T_state) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _get_initial_state_(self) -> D.T_state:\n",
    "        pass\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a `SlipperyMazeDomain` class from this template.\n",
    "We assume that whenever the agent attempts to go up two times in a row, it will go back to its initial state with probability 0.9.\n",
    "Every move costs 1 but 2 when we hit a wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "from copy import deepcopy\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "DEFAULT_MAZE = \"\"\"\n",
    "+-+-+-+-+o+-+-+-+-+-+\n",
    "|   |             | |\n",
    "+ + + +-+-+-+ +-+ + +\n",
    "| | |   |   | | |   |\n",
    "+ +-+-+ +-+ + + + +-+\n",
    "| |   |   | |   |   |\n",
    "+ + + + + + + +-+ +-+\n",
    "|   |   |   | |     |\n",
    "+-+-+-+-+-+-+-+ +-+ +\n",
    "|             |   | |\n",
    "+ +-+-+-+-+ + +-+-+ +\n",
    "|   |       |       |\n",
    "+ + + +-+ +-+ +-+-+-+\n",
    "| | |   |     |     |\n",
    "+ +-+-+ + +-+ + +-+ +\n",
    "| |     | | | |   | |\n",
    "+-+ +-+ + + + +-+ + +\n",
    "|   |   |   |   | | |\n",
    "+ +-+ +-+-+-+-+ + + +\n",
    "|   |       |     | |\n",
    "+-+-+-+-+-+x+-+-+-+-+\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example of State type (adapt to your needs)\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "# Example of Action type (adapt to your needs)\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class D(MDPDomain, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class SlipperyMazeDomain(D):\n",
    "    \n",
    "    def __init__(self, maze_str: str = DEFAULT_MAZE):\n",
    "        maze = []\n",
    "        for y, line in enumerate(maze_str.strip().split(\"\\n\")):\n",
    "            line = line.rstrip()\n",
    "            row = []\n",
    "            for x, c in enumerate(line):\n",
    "                if c in {\" \", \"o\", \"x\"}:\n",
    "                    row.append(1)  # spaces are 1s\n",
    "                    if c == \"o\":\n",
    "                        self._start = State(x, y)\n",
    "                    if c == \"x\":\n",
    "                        self._goal = State(x, y)\n",
    "                else:\n",
    "                    row.append(0)  # walls are 0s\n",
    "            maze.append(row)\n",
    "        # self._render_maze = deepcopy(self._maze)\n",
    "        self._maze = maze\n",
    "        self._num_cols = len(maze[0])\n",
    "        self._num_rows = len(maze)\n",
    "        self._ax = None\n",
    "        self._fig = None\n",
    "        self._image = None\n",
    "    \n",
    "    def _is_terminal(self, state: D.T_state) -> D.T_predicate:\n",
    "        return state == self._goal\n",
    "    \n",
    "    def _get_transition_value(self, memory: D.T_state, action: D.T_event, next_state: Optional[D.T_state] = None) -> Value[D.T_value]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def _get_next_state_distribution(self, memory: D.T_state, action: D.T_event) -> DiscreteDistribution[D.T_state]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        return EnumSpace(Action)\n",
    "    \n",
    "    def _get_applicable_actions_from(self, memory: D.T_state) -> Space[D.T_event]:\n",
    "        transitions = [\n",
    "            (Action.left, State(memory.x - 1, memory.y)),\n",
    "            (Action.right, State(memory.x + 1, memory.y)),\n",
    "            (Action.up, State(memory.x, memory.y - 1)),\n",
    "            (Action.down, State(memory.x, memory.y + 1))\n",
    "        ]\n",
    "        applicable_actions = []\n",
    "        for tr in transitions:\n",
    "            if (\n",
    "                0 <= tr[1].x < self._num_cols\n",
    "                and 0 <= tr[1].y < self._num_rows\n",
    "                and self._maze[tr[1].y][tr[1].x] == 1\n",
    "            ):\n",
    "                applicable_actions.append(tr[0])\n",
    "        return ListSpace(applicable_actions)\n",
    "    \n",
    "    def _get_initial_state_(self) -> D.T_state:\n",
    "        return self._start\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        if self._ax is None:\n",
    "            fig, ax = plt.subplots(1)\n",
    "            fig.canvas.set_window_title(\"Maze\")\n",
    "            ax.set_aspect(\"equal\")  # set the x and y axes to the same scale\n",
    "            plt.xticks([])  # remove the tick marks by setting to an empty list\n",
    "            plt.yticks([])  # remove the tick marks by setting to an empty list\n",
    "            ax.invert_yaxis()  # invert the y-axis so the first row of data is at the top\n",
    "            self._ax = ax\n",
    "            self._fig = fig\n",
    "            plt.ion()\n",
    "        maze = deepcopy(self._maze)\n",
    "        maze[self._goal.y][self._goal.x] = 0.7\n",
    "        maze[memory.y][memory.x] = 0.3\n",
    "        if self._image is None:\n",
    "            self._image = self._ax.imshow(maze)\n",
    "        else:\n",
    "            self._image.set_data(maze)\n",
    "        display(self._fig)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we solve the domain with the Improved LAO* solver, which is an extension of A* to solve Stochastic Shortest Path problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skdecide.hub.solver.ilaostar import ILAOStar\n",
    "from skdecide.utils import rollout\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "domain_factory = lambda: SlipperyMazeDomain()\n",
    "\n",
    "if ILAOStar.check_domain(domain_factory()):\n",
    "    solver_factory = lambda: ILAOStar(\n",
    "        domain_factory=domain_factory,\n",
    "        heuristic=lambda d, s: sqrt((d._goal.x - s.x)**2 + (d._goal.y - s.y)**2),\n",
    "        parallel=False,\n",
    "        debug_logs=False,\n",
    "    )\n",
    "    with solver_factory() as solver:\n",
    "        SlipperyMazeDomain.solve_with(solver, domain_factory)\n",
    "        evaluation_domain = domain_factory()\n",
    "        evaluation_domain.reset()\n",
    "        rollout(\n",
    "            evaluation_domain,\n",
    "            solver,\n",
    "            num_episodes=1,\n",
    "            max_steps=100,\n",
    "            max_framerate=30,\n",
    "            outcome_formatter=lambda o: f\"{o.observation} - cost: {o.value.cost:.2f}\",\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the [match_solvers](https://github.com/airbus/scikit-decide/blob/a7e568951d6198fca19cfc2798d25578824238e6/skdecide/utils.py#L100) function of `scikit-decide`'s utils to get other compatible solvers for our `SlipperyMazeDomain` and solve it using one of those alternative solvers.\n",
    "Try also to get the approximate empirical minimum \"deportation-to-initial-state\" probability that changes the agent's strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq_dec_mak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
