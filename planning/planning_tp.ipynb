{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning course graded exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will solve a probabilistic maze problem defined as a Markov Decision Process ![scikit-decide](https://github.com/airbus/scikit-decide) domain.\n",
    "You will then solve the problem by using one of the probabilistic algorithms provided by `scikit-decide`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to select the right planning class in `sciki-decide`'s ![domain class generator](https://airbus.github.io/scikit-decide/codegen/), i.e. `MPDDomain` which is additionnally `renderable` and have `goals`.\n",
    "It will generate the following domain class template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "\n",
    "\n",
    "# Example of State type (adapt to your needs)\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "# Example of Action type (adapt to your needs)\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class D(MDPDomain, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class MyDomain(D):\n",
    "    \n",
    "    def _is_terminal(self, state: D.T_state) -> D.T_predicate:\n",
    "        pass\n",
    "    \n",
    "    def _get_transition_value(self, memory: D.T_state, action: D.T_event, next_state: Optional[D.T_state] = None) -> Value[D.T_value]:\n",
    "        pass\n",
    "    \n",
    "    def _get_next_state_distribution(self, memory: D.T_state, action: D.T_event) -> SingleValueDistribution[D.T_state]:\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _get_applicable_actions_from(self, memory: D.T_state) -> Space[D.T_event]:\n",
    "        pass\n",
    "    \n",
    "    def _get_goals_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _get_initial_state_(self) -> D.T_state:\n",
    "        pass\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a `SlipperyMazeDomain` class from this template.\n",
    "We assume that whenever the agent attempts to go up two times in a row, it will go back to its initial state with probability 0.9.\n",
    "Every move costs 10 but 20 when we hit a wall or 1 when we go through the light green cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "from copy import deepcopy\n",
    "\n",
    "from skdecide import *\n",
    "from skdecide.builders.domain import *\n",
    "from skdecide.hub.space.gym import EnumSpace, ListSpace\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "DEFAULT_MAZE = \"\"\"\n",
    "+-+-+-+-+o+-+-+-+-+-+\n",
    "|   |             | |\n",
    "+ + + +-+-+-+ +-+ + +\n",
    "| | |   |   | | |   |\n",
    "+ +-+-+ +-+ + + + +-+\n",
    "| |   |   | |   |   |\n",
    "+ + + + + + + +-+ +-+\n",
    "|   |   |   | |     |\n",
    "+-+-+-+-+-+-+-+ +-+ +\n",
    "|             |   | |\n",
    "+ +-+-+-+-+   +-+-+ +\n",
    "|   |      $|       |\n",
    "+ + + +-+ +-+ +-+-+-+\n",
    "| | |         |     |\n",
    "+ +-+-+ + +-+ + +-+ +\n",
    "| |     | | | |   | |\n",
    "+-+ +-+ + + + +-+ + +\n",
    "|   |   |   |   | | |\n",
    "+ +-+ +-+-+-+-+ + + +\n",
    "|   |       |     | |\n",
    "+-+-+-+-+-+x+-+-+-+-+\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example of State type (adapt to your needs)\n",
    "class State(NamedTuple):\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "\n",
    "# Example of Action type (adapt to your needs)\n",
    "class Action(Enum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    left = 2\n",
    "    right = 3\n",
    "\n",
    "\n",
    "class D(MDPDomain, Renderable):\n",
    "    T_state = State  # Type of states\n",
    "    T_observation = T_state  # Type of observations\n",
    "    T_event = Action  # Type of events\n",
    "    T_value = float  # Type of transition values (rewards or costs)\n",
    "    T_info = None  # Type of additional information in environment outcome\n",
    "\n",
    "\n",
    "class SlipperyMazeDomain(D):\n",
    "    \n",
    "    def __init__(self, maze_str: str = DEFAULT_MAZE):\n",
    "        maze = []\n",
    "        for y, line in enumerate(maze_str.strip().split(\"\\n\")):\n",
    "            line = line.rstrip()\n",
    "            row = []\n",
    "            for x, c in enumerate(line):\n",
    "                if c in {\" \", \"o\", \"x\", \"$\"}:\n",
    "                    row.append(1)  # spaces are 1s\n",
    "                    if c == \"o\":\n",
    "                        self._start = State(x, y)\n",
    "                    if c == \"x\":\n",
    "                        self._goal = State(x, y)\n",
    "                    if c == \"$\":\n",
    "                        self._dollar = State(x, y)\n",
    "                else:\n",
    "                    row.append(0)  # walls are 0s\n",
    "            maze.append(row)\n",
    "        # self._render_maze = deepcopy(self._maze)\n",
    "        self._maze = maze\n",
    "        self._num_cols = len(maze[0])\n",
    "        self._num_rows = len(maze)\n",
    "        self._ax = None\n",
    "        self._fig = None\n",
    "        self._image = None\n",
    "    \n",
    "    def _is_terminal(self, state: D.T_state) -> D.T_predicate:\n",
    "        return state == self._goal\n",
    "    \n",
    "    def _get_transition_value(self, memory: D.T_state, action: D.T_event, next_state: Optional[D.T_state] = None) -> Value[D.T_value]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def _get_next_state_distribution(self, memory: D.T_state, action: D.T_event) -> DiscreteDistribution[D.T_state]:\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def _get_action_space_(self) -> Space[D.T_event]:\n",
    "        return EnumSpace(Action)\n",
    "    \n",
    "    def _get_applicable_actions_from(self, memory: D.T_state) -> Space[D.T_event]:\n",
    "        transitions = [\n",
    "            (Action.left, State(memory.x - 1, memory.y)),\n",
    "            (Action.right, State(memory.x + 1, memory.y)),\n",
    "            (Action.up, State(memory.x, memory.y - 1)),\n",
    "            (Action.down, State(memory.x, memory.y + 1))\n",
    "        ]\n",
    "        applicable_actions = []\n",
    "        for tr in transitions:\n",
    "            if (\n",
    "                0 <= tr[1].x < self._num_cols\n",
    "                and 0 <= tr[1].y < self._num_rows\n",
    "                and self._maze[tr[1].y][tr[1].x] == 1\n",
    "            ):\n",
    "                applicable_actions.append(tr[0])\n",
    "        return ListSpace(applicable_actions)\n",
    "    \n",
    "    def _get_goals_(self) -> Space[D.T_observation]:\n",
    "        return ListSpace([self._goal])\n",
    "    \n",
    "    def _get_initial_state_(self) -> D.T_state:\n",
    "        return self._start\n",
    "    \n",
    "    def _get_observation_space_(self) -> Space[D.T_observation]:\n",
    "        pass\n",
    "    \n",
    "    def _render_from(self, memory: D.T_state, **kwargs: Any) -> Any:\n",
    "        if self._ax is None:\n",
    "            fig, ax = plt.subplots(1)\n",
    "            #fig.canvas.set_window_title(\"Maze\")\n",
    "            ax.set_aspect(\"equal\")  # set the x and y axes to the same scale\n",
    "            plt.xticks([])  # remove the tick marks by setting to an empty list\n",
    "            plt.yticks([])  # remove the tick marks by setting to an empty list\n",
    "            ax.invert_yaxis()  # invert the y-axis so the first row of data is at the top\n",
    "            self._ax = ax\n",
    "            self._fig = fig\n",
    "            plt.ion()\n",
    "        maze = deepcopy(self._maze)\n",
    "        maze[self._dollar.y][self._dollar.x] = 0.9\n",
    "        maze[self._goal.y][self._goal.x] = 0.6\n",
    "        maze[memory.y][memory.x] = 0.3\n",
    "        if self._image is None:\n",
    "            self._image = self._ax.imshow(maze)\n",
    "        else:\n",
    "            self._image.set_data(maze)\n",
    "        display(self._fig)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(0.001)\n",
    "        \n",
    "maze = SlipperyMazeDomain()\n",
    "maze.render(maze._get_initial_state_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we solve the domain with the Improved LAO* solver, which is an extension of A* to solve Stochastic Shortest Path problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skdecide.hub.solver.ilaostar import ILAOstar\n",
    "from skdecide.utils import rollout\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "domain_factory = lambda: SlipperyMazeDomain()\n",
    "\n",
    "if ILAOstar.check_domain(domain_factory()):\n",
    "    solver_factory = lambda: ILAOstar(\n",
    "        domain_factory=domain_factory,\n",
    "        heuristic=lambda d, s: Value(cost=sqrt((d._goal.x - s.x)**2 + (d._goal.y - s.y)**2)),\n",
    "        parallel=False,\n",
    "        debug_logs=False,\n",
    "    )\n",
    "    with solver_factory() as solver:\n",
    "        SlipperyMazeDomain.solve_with(solver, domain_factory)\n",
    "        evaluation_domain = domain_factory()\n",
    "        evaluation_domain.reset()\n",
    "        rollout(\n",
    "            evaluation_domain,\n",
    "            solver,\n",
    "            num_episodes=1,\n",
    "            max_steps=100,\n",
    "            max_framerate=30,\n",
    "            outcome_formatter=lambda o: f\"{o.observation} - cost: {o.value.cost:.2f}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the [match_solvers](https://github.com/airbus/scikit-decide/blob/a7e568951d6198fca19cfc2798d25578824238e6/skdecide/utils.py#L100) function of `scikit-decide`'s utils to get other compatible solvers for our `SlipperyMazeDomain` and solve it using one of those alternative solvers.\n",
    "Try also to get the approximate empirical minimum \"deportation-to-initial-state\" probability that changes the agent's strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq_dec_mak",
   "language": "python",
   "name": "seq_dec_mak"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
